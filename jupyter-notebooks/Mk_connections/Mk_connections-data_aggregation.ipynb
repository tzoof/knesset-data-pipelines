{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connections between MKs \n",
    "### **MK = Member of the knesset  (חבר כנסת)\n",
    "Based on transcripts of the knesset committees.<br/>\n",
    "The work was done in the 'public knowledge workshop' hackathon and won 3rd place prize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "rows = []\n",
    "protocol_rows = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting committees texts and analyzing it.\n",
    "We used a library called fataflows since it is the one used by the 'public knowledge workshop' we worked with.<br/>\n",
    "The data download is done in parts - each committee text is divided to parts and processed separately.<br/>\n",
    "Downloading each knesset data and analyzing it took arround 6 hourd per knesset.<br/>\n",
    "This is why we kept a cache of the downloaded data and saved files of the analyzed data for each knesset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit processing of protocol parts for development, -1 means no limit.\n",
    "PROCESS_PARTS_LIMIT = 1\n",
    "\n",
    "# Enable caching of protocol parts data (not efficient, should only be used for local development with sensible PROCESS_PARTS_LIMIT)\n",
    "PROCESS_PARTS_CACHE = True\n",
    "\n",
    "# Filter the meetings to be processed, these kwargs are passed along to DataFlows filter_rows processor for meetings resource\n",
    "MEETINGS_FILTER_ROWS_KWARGS = {'equals': [{'KnessetNum': 20}]}\n",
    "\n",
    "# Don'e use local data - loads everything from knesset data remote storage\n",
    "# When set to False - also enables caching, so you won't download from remote storage on 2nd run.\n",
    "USE_DATA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from url: https://storage.googleapis.com/knesset-data-pipelines/data/members/mk_individual/datapackage.json\n",
      "using cache data from .cache/members-mk-individual-names\n",
      "loading from url: https://storage.googleapis.com/knesset-data-pipelines/data/people/committees/meeting-attendees/datapackage.json\n"
     ]
    }
   ],
   "source": [
    "from dataflows import filter_rows, cache\n",
    "from datapackage_pipelines_knesset.common_flow import load_knesset_data, load_member_names\n",
    "\n",
    "# Loads a dict containing mapping between knesset member id and the member name\n",
    "member_names = load_member_names(use_data=USE_DATA)\n",
    "\n",
    "# define flow steps for loading the source committee meetings data\n",
    "# the actual loading is done later in the Flow\n",
    "load_steps = (\n",
    "    load_knesset_data('people/committees/meeting-attendees/datapackage.json', USE_DATA),\n",
    "    filter_rows(**MEETINGS_FILTER_ROWS_KWARGS)\n",
    ")\n",
    "\n",
    "if not USE_DATA:\n",
    "    # when loading from URL - enable caching which will skip loading on 2nd run\n",
    "    load_steps = (cache(*load_steps, cache_path='.cache/people-committee-meeting-attendees-knesset-20'),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from dataflows import Flow\n",
    "\n",
    "stats = defaultdict(int)\n",
    "member_attended_meetings = defaultdict(int)\n",
    "\n",
    "def to_dataframe(row):\n",
    "    rows.append(row)\n",
    "    \n",
    "def protocols_to_dataframe(row):\n",
    "    protocol_rows.append(row)\n",
    "\n",
    "def process_meeting_protocol_part(row):\n",
    "    stats['processed parts'] += 1\n",
    "\n",
    "def process_meeting(row):\n",
    "    stats['total meetings'] += 1\n",
    "    if row['attended_mk_individual_ids']:\n",
    "        for mk_id in row['attended_mk_individual_ids']:\n",
    "            member_attended_meetings[mk_id] += 1\n",
    "    parts_filename = row['parts_parsed_filename']\n",
    "    if parts_filename:\n",
    "        if PROCESS_PARTS_LIMIT and stats['processed parts'] < PROCESS_PARTS_LIMIT:\n",
    "            steps = (load_knesset_data('committees/meeting_protocols_parts/' + parts_filename, USE_DATA),)\n",
    "            if not USE_DATA and PROCESS_PARTS_CACHE:\n",
    "                steps = (cache(*steps, cache_path='.cache/committee-meeting-protocol-parts/' + parts_filename),)\n",
    "            steps += (process_meeting_protocol_part,)\n",
    "            Flow(*steps).process()\n",
    "\n",
    "process_steps = (to_dataframe,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cache data from .cache/people-committee-meeting-attendees-knesset-20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<datapackage.package.Package at 0x7f7e459e2e80>,\n",
       " {'count_of_rows': 10256,\n",
       "  'bytes': 30129277,\n",
       "  'hash': '9bb63d3b4c724c88df1416113d0fb80c',\n",
       "  'dataset_name': None})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataflows import Flow, dump_to_path\n",
    "\n",
    "Flow(*load_steps, *process_steps, dump_to_path('data/committee-meeting-attendees-parts')).process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get committees data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in enumerate(rows):\n",
    "    parts_filename = row['parts_parsed_filename']\n",
    "    if parts_filename:\n",
    "        if PROCESS_PARTS_LIMIT and stats['processed parts'] < PROCESS_PARTS_LIMIT:\n",
    "            steps = (load_knesset_data('committees/meeting_protocols_parts/' + parts_filename, USE_DATA),)\n",
    "            if not USE_DATA and PROCESS_PARTS_CACHE:\n",
    "                steps = (cache(*steps, cache_path='.cache/committee-meeting-protocol-parts/' + parts_filename),)\n",
    "            steps += (protocols_to_dataframe,)\n",
    "            Flow(*steps).process()\n",
    "    \n",
    "    if len(protocol_rows) > 5:\n",
    "        process_protocol_rows(protocol_rows)\n",
    "        protocol_rows = []\n",
    "\n",
    "        index += 1\n",
    "        if (index + 1) % PROCESS_PARTS_LIMIT == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a graph of MK connections\n",
    "An edge from A to B represents that A spoke to B. <br/>\n",
    "The weight of such edge is the number of times A spoke to B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "def add_edges(speaker, member_set):\n",
    "    if len(member_set) != 0:\n",
    "        for member in member_set:\n",
    "            print ('%s -> %s' % (speaker, member_set))\n",
    "            add_edge(speaker, member)\n",
    "\n",
    "def add_edge(speaker, member):\n",
    "    speaker = speaker[::-1]\n",
    "    member = member[::-1]\n",
    "    if G.has_edge(speaker, member):\n",
    "        G[speaker][member]['weight'] += 1\n",
    "    else:\n",
    "        G.add_edge(speaker,member, weight=1)\n",
    "            \n",
    "def clean(s):\n",
    "    if s is None:\n",
    "        return ''\n",
    "    \n",
    "    cleaned_string = remove_titles(s)\n",
    "    cleaned_string = ''.join([i for i in cleaned_string if i.isalpha() or i.isspace()])\n",
    "    cleaned_string = cleaned_string.strip()\n",
    "    return cleaned_string\n",
    "    \n",
    "def remove_titles(s):\n",
    "    return s.replace('היו\"ר', '')\n",
    "\n",
    "def first_non_speaker(speaker, curr_index, protocol_rows):\n",
    "    i = 0\n",
    "    former_speaker = speaker\n",
    "    while speaker in former_speaker:\n",
    "        i = i + 1\n",
    "        former_speaker = clean(protocol_rows[curr_index - i]['header'])\n",
    "        \n",
    "    return former_speaker\n",
    "\n",
    "def process_protocol_rows(protocol_rows):\n",
    "    \n",
    "    print (protocol_rows[1]['body'])\n",
    "\n",
    "    indirect_reference = ['אתה', 'לך', 'שאתה', ' שאת ']\n",
    "    self_reference = ['אני']\n",
    "    \n",
    "    for index, row in enumerate(protocol_rows[8:]):\n",
    "\n",
    "        if row['header'] is None or row['body'] is None:\n",
    "            print (row)\n",
    "            continue\n",
    "\n",
    "        speaker = clean(row['header'])\n",
    "        spoken_text = clean(row['body'])\n",
    "\n",
    "        direct_references = set([member for member in member_names.values() if (member in spoken_text)])\n",
    "        indirect_references = set([first_non_speaker(speaker, index, protocol_rows) for word in indirect_reference if (word in spoken_text)])\n",
    "        self_reference = set([speaker for word in self_reference if word in spoken_text])\n",
    "\n",
    "        add_edges(speaker, direct_references)\n",
    "        add_edges(speaker, indirect_references)\n",
    "        add_edges(speaker, self_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gml(G, './Mk_connections_graph')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
